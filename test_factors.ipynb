{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Update From 12/29\n",
    "#### Zihan\n",
    "\n",
    "+ revised single factor into generic evaluations - [done]\n",
    "+ for each feature, compute the sharpe ratio for cutoff ranging from 0.5 to 0.9 [done]\n",
    "+ generate binary feature out of the previous computed mappings [done]\n",
    "+ don't think adding article and alert would be helpful - will explain in person\n",
    "+ aggregate sentiment into weekly and biweekly \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import operator\n",
    "\n",
    "with open('sectorMapping.pkl', 'rb') as f:\n",
    "    sectorMapping = pickle.load(f)\n",
    "\n",
    "# google drive link: https://drive.google.com/file/d/1IcuteDnhzD9xfkfVuUJrCEOFiFstzxF_/view?usp=sharing\n",
    "df = pd.read_csv(\"ALL.csv\")\n",
    "df = df[abs(df.returnsOpenNextMktres10) < 0.40] # remove outlier\n",
    "df['sector'] = df.assetCode.apply(lambda x: sectorMapping[x.split(\".\")[0]]) # create sector column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single factor evaluation in a functional way\n",
    "\n",
    "'''\n",
    "helper function for single_factor_eva: top cut_off % short, else long\n",
    "'''\n",
    "def single_factor_helper(df, score, cut_off):\n",
    "    d = {}\n",
    "    sorted_df = df.sort_values(score).reset_index(drop=True)\n",
    "    index = int(len(sorted_df) * cut_off)\n",
    "    short_portfolio = np.mean(sorted_df['returnsOpenNextMktres10'][:index])\n",
    "    long_portfolio = np.mean(sorted_df['returnsOpenNextMktres10'][index:])\n",
    "    d['daily_return'] = long_portfolio - short_portfolio\n",
    "    return pd.Series(d, index=['daily_return'])\n",
    "\n",
    "'''\n",
    "evaluate how well a single factor performs given its column name and a cutoff\n",
    "    e.g. when cutoff is set to be 0.9, it means we short top 10%, and long 90%\n",
    "@param cut_off float\n",
    "@param score str name of the factor\n",
    "@param df dataframe which must contain score column and returnsOpenNextMktres10\n",
    "return NaN only plot and statistics\n",
    "'''\n",
    "def single_factor_eva(df, score, cut_off=0.5, sector_code=None, all_on=False):\n",
    "    \n",
    "    print(\"\\n *** evaluating factor :\", score)\n",
    "    if sector_code != None:\n",
    "        print(\"\\n *** sector: \", sector_code)\n",
    "        factor_return = df.groupby('datetime').apply(lambda x: single_factor_helper(x, score, cut_off))\n",
    "        sector_return = df.groupby('datetime').apply(lambda x: single_factor_helper(x[x.sector == sector_code], score, cut_off))\n",
    "        print(\" *** Overall Sharpe ratio:\", float(factor_return.mean() / factor_return.std()))\n",
    "        print(\" *** Sector Sharpe ratio:\", float(sector_return.mean() / sector_return.std()))\n",
    "        factor_return['cumsum'] = factor_return.daily_return.cumsum()\n",
    "        sector_return['cumsum'] = sector_return.daily_return.cumsum()\n",
    "        plt.figure()\n",
    "        plt.plot(list(factor_return['cumsum']), label = score+\"_\"+str(cut_off))\n",
    "        plt.plot(list(sector_return['cumsum']), label = score+\"_\"+str(cut_off)+\"_sector_\" +str(sector_code))\n",
    "        plt.legend()\n",
    "        plt.savefig(score+\"_\"+str(cut_off)+\"_sector_\" +str(sector_code) +'.png')\n",
    "        plt.show()\n",
    "        return\n",
    "    \n",
    "    if all_on:\n",
    "        plt.figure()\n",
    "        cutoff_sharpe_dict = dict()\n",
    "        df_group = df.groupby('datetime')\n",
    "        for i in [0.1 * i for i in range(1, 10)]:\n",
    "            cutoff_return = df_group.apply(lambda x: single_factor_helper(x, score, i))\n",
    "            cutoff_sharpe_dict[round(i, 1)] = float(cutoff_return.mean() / cutoff_return.std())\n",
    "            cutoff_return['cumsum'] = cutoff_return.daily_return.cumsum()\n",
    "            plt.plot(list(cutoff_return['cumsum']), label = str(int(i * 100))+\"%\")\n",
    "        plt.legend()\n",
    "        title = \"\"\n",
    "        for key in cutoff_sharpe_dict:\n",
    "            title = title + str(key) + \": \" + str(round(cutoff_sharpe_dict[key], 3)) + \"\\n\"\n",
    "        plt.title(score+\":\\n\"+title)\n",
    "        plt.savefig('./cutoff_images_v2/' + score + '.png', bbox_inches='tight')\n",
    "        plt.show()    \n",
    "        return cutoff_sharpe_dict\n",
    "    \n",
    "    factor_return = df.groupby('datetime').apply(lambda x: single_factor_helper(x, score, cut_off))\n",
    "    factor_return['cumsum'] = factor_return.daily_return.cumsum()\n",
    "    plt.figure()\n",
    "    plt.plot(list(factor_return['cumsum']), label = score+\"_\"+str(int(cut_off * 100))+\"%\")\n",
    "    plt.legend()\n",
    "    plt.savefig(score+\"_\"+str(cut_off)+'.png')\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Sector In and Out Performance with various cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "examine_ls = [ 'alert_relevance_weighted_sentiment',\n",
    "       'alert_relevance_weighted_negative_sentiment',\n",
    "       'alert_relevance_weighted_positive_sentiment',\n",
    "       'alert_relevance_weighted_neutral_sentiment',\n",
    "       'alert_news_volume_sum_12H', 'alert_news_novelty_sum_12H',\n",
    "       'alert_news_volume_sum_24H', 'alert_news_novelty_sum_24H',\n",
    "       'alert_news_volume_sum_3D', 'alert_news_novelty_sum_3D',\n",
    "       'alert_news_volume_sum_5D', 'alert_news_novelty_sum_5D',\n",
    "       'alert_news_volume_sum_7D', 'alert_news_novelty_sum_7D',\n",
    "       'article_relevance_weighted_sentiment',\n",
    "       'article_relevance_weighted_negative_sentiment',\n",
    "       'article_relevance_weighted_positive_sentiment',\n",
    "       'article_relevance_weighted_neutral_sentiment',\n",
    "       'article_news_volume_sum_12H', 'article_news_novelty_sum_12H',\n",
    "       'article_news_volume_sum_24H', 'article_news_novelty_sum_24H',\n",
    "       'article_news_volume_sum_3D', 'article_news_novelty_sum_3D',\n",
    "       'article_news_volume_sum_5D', 'article_news_novelty_sum_5D',\n",
    "       'article_news_volume_sum_7D', 'article_news_novelty_sum_7D',\n",
    "       'relevance_weighted_sentiment', 'relevance_weighted_negative_sentiment',\n",
    "       'relevance_weighted_positive_sentiment',\n",
    "       'relevance_weighted_neutral_sentiment', 'news_volume_sum_12H',\n",
    "       'news_novelty_sum_12H', 'news_volume_sum_24H', 'news_novelty_sum_24H',\n",
    "       'news_volume_sum_3D', 'news_novelty_sum_3D', 'news_volume_sum_5D',\n",
    "       'news_novelty_sum_5D', 'news_volume_sum_7D', 'news_novelty_sum_7D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run all sentiment related features and save them in local directory\n",
    "\n",
    "sentiment_cutoff_dict = dict()\n",
    "for i in examine_ls:\n",
    "    sentiment_cutoff_dict[i] = single_factor_eva(df, i, all_on=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_df = pd.DataFrame(sentiment_cutoff_dict) # df is good for column wise operations\n",
    "cutoff_df = cutoff_df.abs()  # extreme is good, sign is not\n",
    "\n",
    "# # uncomment to inspect rank\n",
    "# cutoff_df.max().sort_values(ascending=False) # find top k by mean or max\n",
    "# cutoff_df.mean().sort_values(ascending=False)\n",
    "\n",
    "# maually selected top k most important features\n",
    "top_k_factors = [\n",
    "    'article_relevance_weighted_negative_sentiment',\n",
    "    'article_news_volume_sum_5D',\n",
    "    'article_relevance_weighted_positive_sentiment',\n",
    "    'alert_news_volume_sum_5D',\n",
    "    'alert_relevance_weighted_positive_sentiment',\n",
    "    'alert_relevance_weighted_negative_sentiment',\n",
    "    'article_news_novelty_sum_12H',\n",
    "    'alert_news_novelty_sum_3D'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutoff_lookup test case passed!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "given column name like: 'article_relevance_weighted_negative_sentiment'\n",
    "return the quantile cutoff based on optimal single factor evaluation\n",
    "'''\n",
    "def cutoff_lookup(i):\n",
    "    return cutoff_df[i].sort_values(ascending=False).index[0]\n",
    "\n",
    "# test case for cutoff_lookup function - used min here because original dict had negative value for this feature\n",
    "assert(cutoff_lookup('article_relevance_weighted_negative_sentiment') == \\\n",
    "       min(sentiment_cutoff_dict['article_relevance_weighted_negative_sentiment'].items(), key=operator.itemgetter(1))[0])\n",
    "print(\"cutoff_lookup test case passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary top k sentiment factor passed test!\n"
     ]
    }
   ],
   "source": [
    "# test case for bin_top_k_sentiment_factors\n",
    "\n",
    "'''\n",
    "provide a top_k_factors list of columns names, and df that contains them\n",
    "return only the top k sentiment factors in binary encoding based on their optimal sharpe dict obtained earlier\n",
    "'''\n",
    "def bin_top_k_sentiment_factors(df, top_k_factors):\n",
    "    d = {}\n",
    "    for factor in top_k_factors:\n",
    "        sorted_df = df.sort_values(factor).reset_index(drop=True)\n",
    "        index = int(len(sorted_df) * cutoff_lookup(factor))\n",
    "        index_value = float(sorted_df[factor].iloc[[index]])\n",
    "        d[factor] = df[factor].apply(lambda x: 1 if x >= index_value else -1)\n",
    "    d['returnsOpenNextMktres10'] = df.returnsOpenNextMktres10\n",
    "    d['datetime'] = df.datetime\n",
    "    return pd.DataFrame(d)\n",
    "\n",
    "tmp = df.groupby('datetime').apply(lambda x: bin_top_k_sentiment_factors(x, top_k_factors))\n",
    "\n",
    "def single_factor_helper(x, score):\n",
    "    d = {}\n",
    "    short_portfolio = np.mean(x[x[score] == -1].returnsOpenNextMktres10)\n",
    "    long_portfolio = np.mean(x[x[score] == 1].returnsOpenNextMktres10)\n",
    "    d['daily_return'] = long_portfolio - short_portfolio\n",
    "    return pd.Series(d, index=['daily_return'])\n",
    "\n",
    "e = 10 ** -5\n",
    "ret = tmp.groupby('datetime').apply(lambda x: single_factor_helper(x, 'alert_relevance_weighted_negative_sentiment'))\n",
    "assert(min(sentiment_cutoff_dict['alert_relevance_weighted_negative_sentiment'].values()) - float(ret.mean() / ret.std()) < e)\n",
    "ret = tmp.groupby('datetime').apply(lambda x: single_factor_helper(x, 'alert_relevance_weighted_positive_sentiment'))\n",
    "assert(max(sentiment_cutoff_dict['alert_relevance_weighted_positive_sentiment'].values()) - float(ret.mean() / ret.std()) < e)\n",
    "print(\"binary top k sentiment factor passed test!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.05792754572271414\n",
      "-0.04016692044487653\n"
     ]
    }
   ],
   "source": [
    "ret = tmp.groupby('datetime').apply(lambda x: single_factor_helper(x, 'alert_news_novelty_sum_3D'))\n",
    "print(min(sentiment_cutoff_dict['alert_news_novelty_sum_3D'].values()))\n",
    "print(float(ret.mean() / ret.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# above test failed!\n",
    "# need code review from peers - can't find logic error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo code of running all sector comparison for 'alert_relevance_weighted_positive_sentiment'\n",
    "for i in list(df.sector.unique()):\n",
    "    single_factor_eva(df, 'alert_relevance_weighted_positive_sentiment', 0.5, i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
